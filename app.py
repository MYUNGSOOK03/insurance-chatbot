"""
ë³´í—˜ì•½ê´€ ì±—ë´‡ - RAG ê¸°ë°˜ Q&A ì‹œìŠ¤í…œ
"""
import streamlit as st
import os
from pypdf import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¤– ë³´í—˜ì•½ê´€ ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

# ì œëª©
st.title("ğŸ¤– ë³´í—˜ì•½ê´€ ì±—ë´‡")
st.markdown("**ë‹¤ì´ë ‰íŠ¸ ì›°ë¹™ ê±´ê°•ë³´í—˜** ì•½ê´€ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”!")

# OpenAI API í‚¤ ì…ë ¥
api_key = st.sidebar.text_input("OpenAI API Key", type="password")

if not api_key:
    st.warning("âš ï¸ ì™¼ìª½ ì‚¬ì´ë“œë°”ì— OpenAI API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.info("API KeyëŠ” https://platform.openai.com/api-keys ì—ì„œ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    st.stop()

os.environ["OPENAI_API_KEY"] = api_key

# PDF íŒŒì¼ ê²½ë¡œ (Streamlit Cloudìš© ìƒëŒ€ ê²½ë¡œ)
import os
# PDF íŒŒì¼ ê²½ë¡œ
PDF_FILE = "insurance_policy.pdf"

# íŒŒì¼ ì¡´ì¬ í™•ì¸
if not os.path.exists(PDF_FILE):
    st.error(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PDF_FILE}")
    st.info(f"í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}")
    st.info(f"ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼: {os.listdir('.')}")
    st.stop()

@st.cache_resource
def load_pdf_and_create_vectorstore():
    """PDFë¥¼ ë¡œë“œí•˜ê³  ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±"""
    try:
        # PDF íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(PDF_FILE)
        st.info(f"ğŸ“„ PDF íŒŒì¼ í¬ê¸°: {file_size / 1024 / 1024:.2f} MB")
        
        if file_size == 0:
            st.error("âŒ PDF íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return None, 0
        
        # PDF ì½ê¸°
        pdf_reader = PdfReader(PDF_FILE)
        st.info(f"ğŸ“– PDF í˜ì´ì§€ ìˆ˜: {len(pdf_reader.pages)}")
        
        text = ""
        for i, page in enumerate(pdf_reader.pages):
            page_text = page.extract_text()
            text += page_text
            if i == 0:  # ì²« í˜ì´ì§€ ìƒ˜í”Œ ì¶œë ¥
                st.info(f"ì²« í˜ì´ì§€ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: {page_text[:100]}...")
        
        if not text.strip():
            st.error("âŒ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None, 0
        
        st.info(f"ğŸ“ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)} ë¬¸ì")
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        chunks = text_splitter.split_text(text)
        st.info(f"âœ‚ï¸ ì²­í¬ ë¶„í•  ì™„ë£Œ: {len(chunks)}ê°œ")
        
        # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±
        embeddings = OpenAIEmbeddings()
        vectorstore = FAISS.from_texts(chunks, embeddings)
        
        return vectorstore, len(chunks)
    except Exception as e:
        st.error(f"âŒ PDF ë¡œë”© ì¤‘ ì˜¤ë¥˜: {str(e)}")
        import traceback
        st.error(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        return None, 0

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
with st.spinner("ğŸ“„ ë³´í—˜ì•½ê´€ ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
    vectorstore, num_chunks = load_pdf_and_create_vectorstore()

if vectorstore:
    st.sidebar.success(f"âœ… ë¬¸ì„œ ë¡œë”© ì™„ë£Œ! ({num_chunks}ê°œ ì²­í¬)")
    
    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    # LLM ì„¤ì •
    llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo")
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. 
    ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´, ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.
    
    ì»¨í…ìŠ¤íŠ¸: {context}
    
    ì§ˆë¬¸: {question}
    
    ë‹µë³€:"""
    
    prompt = ChatPromptTemplate.from_template(template)
    
    # RAG ì²´ì¸ ìƒì„±
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    st.markdown("---")
    
    # ì˜ˆì‹œ ì§ˆë¬¸
    st.sidebar.markdown("### ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸")
    example_questions = [
        "ì…ì›ë¹„ ì²­êµ¬ì— í•„ìš”í•œ ì„œë¥˜ëŠ”?",
        "ë³´ì¥ ê°œì‹œì¼ì€ ì–¸ì œì¸ê°€ìš”?",
        "ì•” ì§„ë‹¨ë¹„ëŠ” ì–¼ë§ˆë‚˜ ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?",
        "ë©´ì±…ê¸°ê°„ì´ ë¬´ì—‡ì¸ê°€ìš”?",
        "ë³´í—˜ë£Œ ë‚©ì…ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"
    ]
    
    for q in example_questions:
        if st.sidebar.button(q, key=f"example_{q}"):
            st.session_state.current_question = q
    
    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if "sources" in message:
                with st.expander("ğŸ“š ì°¸ê³  ê·¼ê±°"):
                    for i, source in enumerate(message["sources"], 1):
                        st.markdown(f"**[ê·¼ê±° {i}]**\n{source[:300]}...")
    
    # ì§ˆë¬¸ ì…ë ¥
    question = st.chat_input("ë³´í—˜ì•½ê´€ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”...")
    
    # ì˜ˆì‹œ ì§ˆë¬¸ í´ë¦­ ì‹œ
    if "current_question" in st.session_state:
        question = st.session_state.current_question
        del st.session_state.current_question
    
    if question:
        # ì‚¬ìš©ì ì§ˆë¬¸ í‘œì‹œ
        st.session_state.chat_history.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(question)
        
        # AI ë‹µë³€ ìƒì„±
        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                try:
                    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
                    docs = retriever.invoke(question)
                    context = format_docs(docs)
                    
                    # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
                    chain = (
                        {"context": lambda x: context, "question": RunnablePassthrough()}
                        | prompt
                        | llm
                        | StrOutputParser()
                    )
                    
                    answer = chain.invoke(question)
                    
                    st.markdown(answer)
                    
                    # ê·¼ê±° í‘œì‹œ
                    if docs:
                        sources = [doc.page_content for doc in docs]
                        with st.expander("ğŸ“š ì°¸ê³  ê·¼ê±°"):
                            for i, source in enumerate(sources, 1):
                                st.markdown(f"**[ê·¼ê±° {i}]**\n{source[:300]}...")
                        
                        st.session_state.chat_history.append({
                            "role": "assistant",
                            "content": answer,
                            "sources": sources
                        })
                    else:
                        st.session_state.chat_history.append({
                            "role": "assistant",
                            "content": answer
                        })
                except Exception as e:
                    st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
    if st.sidebar.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.chat_history = []
        st.rerun()

else:
    st.error("âŒ ë¬¸ì„œ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°” ì •ë³´
st.sidebar.markdown("---")
st.sidebar.markdown("""
### â„¹ï¸ ì‚¬ìš© ë°©ë²•
1. OpenAI API Key ì…ë ¥
2. ë³´í—˜ì•½ê´€ì— ëŒ€í•´ ì§ˆë¬¸ ì…ë ¥
3. AIê°€ ê·¼ê±°ì™€ í•¨ê»˜ ë‹µë³€ ì œê³µ

### ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ
- LangChain (RAG)
- FAISS (ë²¡í„° DB)
- OpenAI GPT-3.5
- Streamlit
""")
